# Local DeepSeek Chat with OllamaÂ +Â Streamlit

**Run DeepSeekâ€‘R1 entirely offline with Ollama and give it a clean Streamlit interface â€” all developed from PyCharm under WSL.**

---

## Features

| Stack             | What it does                                                         |
| ----------------- | -------------------------------------------------------------------- |
| **Ollama**        | Pulls & serves the DeepSeekâ€‘R1Â 14B model on `localhost:11434`.       |
| **Streamlit**     | Lightweight web UI with live token streaming and â€œthinkingâ€ display. |
| **WSLÂ +Â PyCharm** | Linuxâ€native toolchain while coding comfortably on Windows.          |

---

## ðŸ–¥Â Quick terminal chat

```bash
# Pull the model (â‰ˆÂ 9Â GB once):
ollama pull deepseek-r1:14b

# Start the server (default port 11434)
ollama serve

# Chat from another shell
ollama run deepseek-r1:14b -- "Explain the FastÂ InverseÂ SquareÂ Root trick"
```

---

## Run the Streamlit web app

```bash
# Clone this repo
git clone https://github.com/YOURâ€‘USER/deepseek-streamlit-chat.git
cd deepseek-streamlit-chat

# Create an isolated env (avoids PEPÂ 668 issues)
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Ollama server must still be running in a separate shell
streamlit run app.py
```

Browse to [http://localhost:8501](http://localhost:8501) and start chatting. While the LLM thinks the UI shows a faded *Thinkingâ€¦* placeholder, then streams the final answer tokenâ€‘byâ€‘token.

---

## Architecture

```
User â†’ Streamlit UI (WSL) â†’ Ollama (DeepSeekâ€‘R1)
```

Two small processes; nothing else required.

---

## Full WSLÂ +Â PyCharm setup

1. **Install WSLÂ &Â Ubuntu**

   ```powershell
   wsl --install -d Ubuntu
   ```
2. **Inside Ubuntu**

   ```bash
   # Ollama
   curl https://ollama.ai/install.sh | sh
   ollama pull deepseek-r1:14b

   # Dev tools
   sudo apt update && sudo apt install zip python3-venv

   # Clone project
   git clone https://github.com/YOURâ€‘USER/deepseek-streamlit-chat.git ~/project
   cd ~/project
   python3 -m venv .venv && source .venv/bin/activate
   pip install -r requirements.txt
   ```
3. **PyCharm**

   * Open `~/project` as a WSL project.
   * Settings â–¸ PythonÂ Interpreter â–¸ **Existing** â†’ `.venv/bin/python`.
   * Run configuration â†’ `streamlit run app.py`.

---

## Deploy on a small VPS

```bash
# /etc/systemd/system/ollama.service
[Service]
ExecStart=/usr/bin/ollama serve
Restart=always
Environment="OLLAMA_HOST=127.0.0.1:11434"

# /etc/systemd/system/streamlit.service
[Service]
WorkingDirectory=/opt/deepseek-streamlit-chat
ExecStart=/opt/deepseek-streamlit-chat/.venv/bin/streamlit run app.py \
         --server.port 8501 --server.headless true --server.enableCORS false
Restart=always

sudo systemctl enable --now ollama streamlit
```

Place Nginx / Caddy in front of portÂ 8501 for HTTPS.

---

## DeveloperÂ workflow

| Task                  | Command                                                        |
| --------------------- | -------------------------------------------------------------- |
| Start dev server      | `streamlit run app.py`                                         |
| Update DeepSeek model | `ollama pull deepseek-r1:16b`                                  |
| Freeze deps           | `pip freeze > requirements.txt`                                |
| Make a release zip    | `zip -r ../release.zip . -x '*.pyc' '*__pycache__*' '.venv/*'` |


---

## ðŸ“„Â License

MITÂ Â©Â 2025Â Enze
